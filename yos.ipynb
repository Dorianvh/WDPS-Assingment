{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Api notebook\n",
    "\n",
    "API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "\n",
    "def get_entity_data(entity_id, languages='en') -> dict:\n",
    "    \"\"\"collect relevant information on an entity such as sitelinks, statements and description\n",
    "\n",
    "    Args:\n",
    "        entity_id (string): id of an entity\n",
    "        languages (str, optional)\n",
    "\n",
    "    Returns:\n",
    "        dict: returns a detailed dictionairy of a given entity\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(API_URL, params= { \"action\": \"wbgetentities\", \"ids\": entity_id, \"format\": \"json\", \"languages\": languages })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        if \"entities\" in data and entity_id in data[\"entities\"]:\n",
    "            entity = data.get('entities').get(entity_id)\n",
    "\n",
    "            # Get label and description of entity\n",
    "            label = entity['labels']['en']['value']\n",
    "            description = entity['descriptions']['en']['value']\n",
    "\n",
    "\n",
    "            # Get the number of linked sites\n",
    "            num_sitelinks =  len(entity['sitelinks'].keys())\n",
    "\n",
    "            # Wikepedia url\n",
    "\n",
    "            # Get the sites linked to this entity\n",
    "            wikipedia_url = entity['sitelinks'].get('enwiki')\n",
    "\n",
    "            if wikipedia_url:\n",
    "                url = 'https://en.wikipedia.org/wiki/' + wikipedia_url['title'].replace(' ', '_')\n",
    "            else:\n",
    "                url = ''\n",
    "\n",
    "            # Claims\n",
    "            \n",
    "            claims_data = {}\n",
    "\n",
    "            # Extract property values and labels\n",
    "            for key, claim_list in entity['claims'].items():\n",
    "                values = []\n",
    "\n",
    "                for claim in claim_list:\n",
    "                    val = claim.get('mainsnak', {}).get('datavalue', {}).get('value', {})\n",
    "                    values.append(val)\n",
    "\n",
    "                claims_data[key] = values\n",
    "\n",
    "\n",
    "            return {'label': label, 'description': description, 'claims': claims_data, 'sitelinks': num_sitelinks, 'url': url}\n",
    "        \n",
    "    return {}\n",
    "\n",
    "\n",
    "\n",
    "def get_entities(entity, language=\"en\", limit=5) -> list:\n",
    "    \"\"\"get a list of candidates for a specific entity\n",
    "\n",
    "    Args:\n",
    "        entity (string): name of the entity you want to search\n",
    "        language (str, optional): Defaults to \"en\".\n",
    "        limit (int, optional): Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: list of IDs found for this entity\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(API_URL, params={ \"action\": \"wbsearchentities\", \"search\": entity, \"language\": language, \"format\": \"json\", \"limit\": limit, })\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        candidates = []\n",
    "\n",
    "        for entity in data.get(\"search\", []):\n",
    "            candidates.append(entity[\"id\"])\n",
    "\n",
    "        return candidates\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decypher_label(entity_id):\n",
    "    response = requests.get(API_URL, params= { \"action\": \"wbgetentities\", \"ids\": entity_id, \"format\": \"json\", \"languages\": 'en' })\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        if \"entities\" in data and entity_id in data[\"entities\"]:\n",
    "            entity = data.get('entities').get(entity_id)\n",
    "\n",
    "            return entity['labels']['en']['value']\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def get_statements(entity):\n",
    "\n",
    "    claims = {}\n",
    "\n",
    "    for property_id, property_values in entity['claims'].items():\n",
    "        \n",
    "        property_label = decypher_label(property_id)\n",
    "\n",
    "        values = []\n",
    "        \n",
    "        for value in property_values:\n",
    "            \n",
    "            # If the value is an entity, resolve its label\n",
    "            if isinstance(value, dict) and \"id\" in value:\n",
    "                value_label = decypher_label(value[\"id\"])\n",
    "                values.append(value_label)\n",
    "            else:\n",
    "                values.append(value)\n",
    "        \n",
    "        # Get human-readable label for property ID\n",
    "        claims[property_label] = values\n",
    "\n",
    "        break\n",
    "    \n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'occupation': ['poet', 'author', 'writer']}\n"
     ]
    }
   ],
   "source": [
    "query = 'homer'\n",
    "\n",
    "candidates = get_entities(query)\n",
    "\n",
    "for candidate in candidates[2:]:\n",
    "    y = get_entity_data(candidate)\n",
    "    print(get_statements(y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Question: Is it correct that Berlin is the capital of Germany?\n",
      "[]\n",
      "\n",
      "Processing Question: Does Japan have the highest life expectancy in the world?\n",
      "[('Japan', 'have', 'expectancy')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Wikidata API endpoint\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def search_wikidata(entity):\n",
    "    \"\"\"Search for an entity in Wikidata and return its Wikidata ID.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": entity,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"search\" in data and len(data[\"search\"]) > 0:\n",
    "            return data[\"search\"][0][\"id\"]  # Return the first result's ID\n",
    "    return None\n",
    "\n",
    "def get_relation_from_wikidata(subject_id, relation_label):\n",
    "    \"\"\"Retrieve objects linked to a subject by a relation in Wikidata.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": subject_id,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"entities\" in data and subject_id in data[\"entities\"]:\n",
    "            claims = data[\"entities\"][subject_id][\"claims\"]\n",
    "            results = []\n",
    "            for property_id, statements in claims.items():\n",
    "                # Match the relation by label\n",
    "                if relation_label.lower() in get_label(property_id).lower():\n",
    "                    for statement in statements:\n",
    "                        mainsnak = statement.get(\"mainsnak\", {})\n",
    "                        datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                        if datavalue.get(\"type\") == \"wikibase-entityid\":\n",
    "                            object_id = datavalue[\"value\"][\"id\"]\n",
    "                            results.append(get_label(object_id))\n",
    "            return results\n",
    "    return []\n",
    "\n",
    "def get_label(entity_id):\n",
    "    \"\"\"Fetch the label of a Wikidata entity.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": entity_id,\n",
    "        \"format\": \"json\",\n",
    "        \"languages\": \"en\",\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"entities\" in data and entity_id in data[\"entities\"]:\n",
    "            return data[\"entities\"][entity_id].get(\"labels\", {}).get(\"en\", {}).get(\"value\", entity_id)\n",
    "    return entity_id  # Fallback to entity ID if label not found\n",
    "\n",
    "def process_question(question):\n",
    "    \"\"\"Extract entities, relations, and verify with Wikidata.\"\"\"\n",
    "    doc = nlp(question)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    relations = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"ROOT\", \"attr\", \"prep\") and token.head.pos_ in (\"VERB\", \"AUX\"):\n",
    "            # Extract relations\n",
    "            subject = [child for child in token.head.lefts if child.dep_ == \"nsubj\"]\n",
    "            object_ = [child for child in token.head.rights if child.dep_ in (\"pobj\", \"dobj\")]\n",
    "            if subject and object_:\n",
    "                relations.append((subject[0].text, token.text, object_[0].text))\n",
    "\n",
    "    print([ent.text for ent in doc.ents])\n",
    "    # # Link entities to Wikidata and verify relations\n",
    "    # for subject, relation, object_ in relations:\n",
    "    #     subject_id = search_wikidata(subject)\n",
    "    #     if subject_id:\n",
    "    #         objects_from_kg = get_relation_from_wikidata(subject_id, relation)\n",
    "    #         if object_ in objects_from_kg:\n",
    "    #             print(f\"Relation Verified: {subject} {relation} {object_}\")\n",
    "    #         else:\n",
    "    #             print(f\"Relation Not Verified: {subject} {relation} {object_}\")\n",
    "    #     else:\n",
    "    #         print(f\"Entity not found in Wikidata: {subject}\")\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"Is it correct that Berlin is the capital of Germany?\",\n",
    "    \"Does Japan have the highest life expectancy in the world?\",\n",
    "]\n",
    "\n",
    "# Process each question\n",
    "for question in questions:\n",
    "    print(f\"Processing Question: {question}\")\n",
    "    process_question(question)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is it correct that Berlin is the capital of Germany?\n",
      "Entities: [('Berlin', 'GPE'), ('Germany', 'GPE')]\n",
      "Relations: [('it', 'Is', 'correct'), ('Berlin', 'is', 'capital'), ('capital', 'of', 'Germany')]\n",
      "\n",
      "Question: Does Japan have the highest life expectancy in the world?\n",
      "Entities: [('Japan', 'GPE')]\n",
      "Relations: [('Japan', 'have', 'expectancy'), ('expectancy', 'in', 'world')]\n",
      "\n",
      "Question: True or False: Brazil’s official language is Portuguese.\n",
      "Entities: [('Brazil', 'GPE'), ('Portuguese', 'NORP')]\n",
      "Relations: [('language', 'is', 'Portuguese')]\n",
      "\n",
      "Question: Would it be accurate to say the Eiffel Tower is in Paris?\n",
      "Entities: [('the Eiffel Tower', 'LOC'), ('Paris', 'GPE')]\n",
      "Relations: [('it', 'be', 'accurate')]\n",
      "\n",
      "Question: Does Australia function as both a country and a continent?\n",
      "Entities: [('Australia', 'GPE')]\n",
      "Relations: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities_and_relations(sentence):\n",
    "    \"\"\"Extract entities and relations using dependency parsing.\"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    entities = []\n",
    "    relations = []\n",
    "\n",
    "    # Extract entities using NER\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "\n",
    "    # Dependency-based relation extraction\n",
    "    for token in doc:\n",
    "        # Identify root verbs and their subjects/objects\n",
    "        if token.pos_ in (\"VERB\", \"AUX\"):\n",
    "            subject = [child for child in token.children if child.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "            objects = [child for child in token.children if child.dep_ in (\"dobj\", \"pobj\", \"attr\", \"acomp\")]\n",
    "\n",
    "            if subject and objects:\n",
    "                for subj in subject:\n",
    "                    for obj in objects:\n",
    "                        relations.append((subj.text, token.text, obj.text))\n",
    "\n",
    "        # Extract prepositional relations (e.g., \"capital of Germany\")\n",
    "        if token.dep_ == \"prep\" and token.head.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            obj = [child for child in token.children if child.dep_ == \"pobj\"]\n",
    "            if obj:\n",
    "                relations.append((token.head.text, token.text, obj[0].text))\n",
    "\n",
    "    return entities, relations\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"Is it correct that Berlin is the capital of Germany?\",\n",
    "    \"Does Japan have the highest life expectancy in the world?\",\n",
    "    \"True or False: Brazil’s official language is Portuguese.\",\n",
    "    \"Would it be accurate to say the Eiffel Tower is in Paris?\",\n",
    "    \"Does Australia function as both a country and a continent?\",\n",
    "]\n",
    "\n",
    "# Process each question\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    entities, relations = extract_entities_and_relations(question)\n",
    "    print(f\"Entities: {entities}\")\n",
    "    print(f\"Relations: {relations}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Berlin', 'GPE') be Is\n",
      "('Germany', 'GPE') be Is\n",
      "('Berlin', 'GPE') it Is\n",
      "('Germany', 'GPE') it Is\n",
      "('Berlin', 'GPE') Berlin is\n",
      "('Germany', 'GPE') Berlin is\n",
      "('Berlin', 'GPE') capital is\n",
      "('Germany', 'GPE') capital is\n",
      "('Japan', 'GPE') Japan have\n",
      "('Japan', 'GPE') have have\n",
      "('Brazil', 'GPE') language is\n",
      "('Portuguese', 'NORP') language is\n",
      "('Brazil', 'GPE') be is\n",
      "('Portuguese', 'NORP') be is\n",
      "('the Eiffel Tower', 'LOC') it be\n",
      "('Paris', 'GPE') it be\n",
      "('the Eiffel Tower', 'LOC') be be\n",
      "('Paris', 'GPE') be be\n",
      "('the Eiffel Tower', 'LOC') Tower is\n",
      "('Paris', 'GPE') Tower is\n",
      "('Australia', 'GPE') Australia function\n",
      "('Australia', 'GPE') function function\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of questions\n",
    "questions = [\n",
    "    \"Is it correct that Berlin is the capital of Germany?\",\n",
    "    \"Does Japan have the highest life expectancy in the world?\",\n",
    "    \"True or False: Brazil's official language is Portuguese.\",\n",
    "    \"Would it be accurate to say the Eiffel Tower is in Paris?\",\n",
    "    \"Does Australia function as both a country and a continent?\"\n",
    "]\n",
    "\n",
    "\n",
    "def perform_ner(text):\n",
    "    \"\"\"\n",
    "    Perform Named Entity Recognition on the input text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        list: List of extracted named entities with their types\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def extract_relations(text, entities):\n",
    "    \"\"\"\n",
    "    Extract potential relations between entities\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        entities (list): List of named entities\n",
    "    \n",
    "    Returns:\n",
    "        list: List of potential relations\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    \n",
    "    # Simple relation extraction based on dependencies\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj', 'attr', 'ROOT']:\n",
    "            for ent in entities:\n",
    "                # if ent[0] in token.subtree.text:\n",
    "                    \n",
    "                    print(ent, token.lemma_, token.head.text)\n",
    "    \n",
    "    return relations\n",
    "\n",
    "# Process each question\n",
    "results = []\n",
    "\n",
    "for question in questions:\n",
    "    # Perform NER\n",
    "    entities = perform_ner(question)\n",
    "    \n",
    "    # Extract relations\n",
    "    relations = extract_relations(question, entities)\n",
    "    \n",
    "\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(\"\\nQuestion:\", result['question'])\n",
    "    print(\"Entities:\", result['entities'])\n",
    "    print(\"Relations:\", result['relations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin --> is --> the capital\n",
      "the capital --> of --> Germany\n",
      "Japan --> have --> the highest life expectancy\n",
      "the highest life expectancy --> in --> the world\n",
      "is --> in --> Paris\n",
      "function --> as --> both a country\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Merge noun phrases and entities for easier analysis\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "TEXTS = [\n",
    "    \"Is it correct that Berlin is the capital of Germany?\",\n",
    "    \"Does Japan have the highest life expectancy in the world?\",\n",
    "    \"True or False: Brazil's official language is Portuguese.\",\n",
    "    \"Would it be accurate to say the Eiffel Tower is in Paris?\",\n",
    "    \"Does Australia function as both a country and a continent?\"\n",
    "]\n",
    "\n",
    "# Process the texts\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    for token in doc:\n",
    "        # Checking for subjects and objects for general relations\n",
    "        if token.dep_ in (\"nsubj\", \"nsubjpass\"):  # Subject\n",
    "            # Look for the verb (head of subject)\n",
    "            verb = token.head\n",
    "            # Find the object (direct or prepositional)\n",
    "            objects = [child for child in verb.children if child.dep_ in (\"dobj\", \"pobj\", \"attr\")]\n",
    "\n",
    "            # If we found an object, print the subject, verb, and object\n",
    "            if objects:\n",
    "                for obj in objects:\n",
    "                    print(f\"{token.text} --> {verb.text} --> {obj.text}\")\n",
    "\n",
    "        # Check for prepositional phrases (e.g., 'capital of Germany')\n",
    "        if token.dep_ == \"prep\":\n",
    "            # Get the head of the preposition (the noun that the prep modifies)\n",
    "            head = token.head\n",
    "            # Get the object of the preposition\n",
    "            pobj = [child for child in token.children if child.dep_ == \"pobj\"]\n",
    "            if pobj:\n",
    "                for obj in pobj:\n",
    "                    print(f\"{head.text} --> {token.text} --> {obj.text}\")\n",
    "\n",
    "        # Check for entities of type MONEY or other types that are relevant\n",
    "        if token.ent_type_ in (\"MONEY\", \"GPE\", \"ORG\", \"LOC\"):\n",
    "            # Direct object or subject relations\n",
    "            if token.dep_ in (\"attr\", \"dobj\", \"pobj\"):\n",
    "                subj = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "                if subj:\n",
    "                    print(subj[0], \"-->\", token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
